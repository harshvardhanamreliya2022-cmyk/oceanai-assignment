# ==================== LLM Configuration ====================

# LLM Provider: Choose one (groq, ollama, or openai)
LLM_PROVIDER=groq

# Groq Configuration (Recommended - Free tier available)
# Get your API key from: https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=mixtral-8x7b-32768

# Ollama Configuration (Local option - No API key needed)
# Install from: https://ollama.ai/download
# Run: ollama pull llama3
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# OpenAI Configuration (Optional - Paid)
# Get your API key from: https://platform.openai.com
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview

# ==================== Vector Database Configuration ====================

# Path to ChromaDB persistent storage
VECTORDB_PATH=./data/vectordb

# Embedding model from Sentence Transformers
# Options: all-MiniLM-L6-v2, all-mpnet-base-v2
EMBEDDING_MODEL=all-MiniLM-L6-v2

# ==================== Application Settings ====================

# Directory paths (relative to project root)
UPLOAD_DIR=./data/uploads
SCRIPTS_DIR=./data/scripts
LOGS_DIR=./data/logs

# Text Chunking Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# ==================== API Configuration ====================

# Backend API URL (for frontend to connect)
BACKEND_URL=http://localhost:8000

# Frontend Port
FRONTEND_PORT=8501

# Backend Port
BACKEND_PORT=8000

# ==================== Logging Configuration ====================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file rotation
LOG_MAX_BYTES=10485760  # 10MB
LOG_BACKUP_COUNT=5

# ==================== Performance Configuration ====================

# Maximum file upload size (in bytes)
MAX_UPLOAD_SIZE=10485760  # 10MB

# Maximum number of chunks to retrieve for RAG
MAX_RETRIEVAL_CHUNKS=5

# LLM Generation Settings
LLM_TEMPERATURE=0.0  # 0.0 = deterministic, 1.0 = creative
LLM_MAX_TOKENS=2000

# ==================== Security Settings ====================

# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:8501,http://127.0.0.1:8501

# API rate limiting (requests per minute)
RATE_LIMIT_PER_MINUTE=60

# ==================== Feature Flags ====================

# Enable/disable features
ENABLE_NEGATIVE_TESTS=true
ENABLE_SCRIPT_VALIDATION=true
ENABLE_SYNTAX_HIGHLIGHTING=true
# QA Agent Environment Configuration
# Copy this file to .env and fill in your values

# =================================
# LLM Provider Configuration
# =================================
# Choose your LLM provider: groq, ollama, or openai
LLM_PROVIDER=groq

# Groq Configuration (recommended)
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=mixtral-8x7b-32768

# Ollama Configuration (local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# OpenAI Configuration (optional)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4-turbo-preview

# =================================
# Application Settings
# =================================
DEBUG=False
LOG_LEVEL=INFO

# =================================
# Paths (defaults are usually fine)
# =================================
VECTORDB_PATH=./data/vectordb
UPLOAD_DIR=./data/uploads
SCRIPTS_DIR=./data/scripts
LOG_DIR=./data/logs

# =================================
# Processing Settings
# =================================
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_RETRIEVAL=5
